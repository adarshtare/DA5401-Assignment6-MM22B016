# DA5401 A6 â€” Imputation via Regression for Missing Data

This repository contains the complete workflow for **DA5401 Assignment 6**, focused on introducing missingness and comparing **simple**, **linear-regression**, and **nonâ€‘linear regression** imputation strategies by evaluating downstream **creditâ€‘default** classification performance.

## ğŸ“Œ Objective
1. Introduce **Missing At Random (MAR)** values (â‰ˆ5â€“10%) in a few numeric columns.
2. Construct clean datasets using three imputation strategies (**Median**, **Linear Regression**, **Nonâ€‘Linear Regression**) and a **Listwise Deletion** baseline.
3. Train a **Logistic Regression** classifier on each dataset and compare performance (Accuracy, Precision, Recall, F1).
4. Provide a short analysis + recommendation on which strategy is best and why.

---
## ğŸ“‚ Folder Structure  

â”œâ”€â”€ DA5401_Assignment6_MM22B016.ipynb  # Main Jupyter Notebook  
â”œâ”€â”€ README.md                          # Documentation file  
â””â”€â”€ UCI_Credit_Card.csv                # Dataset

## ğŸ“‚ Dataset
- **Source:** UCI Credit Card Default Clients (30,000 Ã— 25).
- **Target label:** `default.payment.next.month` (0/1).
- **File used in Colab:** `UCI_Credit_Card.csv` (uploaded via Files pane).

### Columns we perturbed with MAR
- `AGE`, `BILL_AMT1`, `PAY_AMT1` (â‰ˆ7% each)
- Target column is **never** perturbed.

### Column imputed via regression (B & C)
- **Chosen column:** `AGE`  
  - **B:** Linear Regression  
  - **C:** Nonâ€‘Linear Regression (KNN Regressor, k=5)

> **Professorâ€™s clarification respected:** For B & C, **only one** column is imputed by regression; **other NaNs remain unchanged**. Using one perturbed column is considered an **individual imputation experiment**.

---

## ğŸ§ª Part A â€” Data Preprocessing & Imputation

### A1. Introduce MAR
Randomly blank â‰ˆ7% values in `AGE`, `BILL_AMT1`, `PAY_AMT1`.

### A2. Strategy 1 â€” Median Imputation â†’ **Dataset A**
- Medianâ€‘impute **only the columns that contain NaNs** (never the label).
- **Why median over mean?** Robust to outliers and skew (typical of bill/payment amounts).

### A3. Strategy 2 â€” Regression Imputation (Linear) â†’ **Dataset B**
- Impute **only `AGE`** using **Linear Regression** trained on other features.
- **Assumption:** **MAR** â€” missingness depends only on observed variables, not on the unobserved value itself.

### A4. Strategy 3 â€” Regression Imputation (Nonâ€‘Linear) â†’ **Dataset C**
- Impute **only `AGE`** using **KNN Regressor (k=5)**.
- **Assumption:** Same **MAR**; nonâ€‘linear model can capture curved/threshold relations if present.

### A5. Dataset D â€” Listwise Deletion
- Drop any row with **any** missing value.

---

## âš™ï¸ Part B â€” Model Training & Assessment

### B1. Train/Test Split
- Stratified 80/20 split for each dataset (A, B, C, D).

### B2. Standardization
- Same pipeline for all models: `SimpleImputer(median) â†’ StandardScaler â†’ LogisticRegression`  
  (This is **trainâ€‘time preprocessing only**, not a change to B/C rules.)

### B3. Classifier
- `LogisticRegression(max_iter=500, random_state=42)`

---

## ğŸ“Š Part C â€” Comparative Analysis

### Summary Performance (your run)
| Model | Precision (Macro) | Recall (Macro) | F1 (Macro) | Accuracy |
|:--|:--:|:--:|:--:|:--:|
| A â€” Median Imputation | **0.7546** | 0.6053 | 0.6225 | 0.8083 |
| B â€” Linear Regression Imputation | 0.7531 | 0.6044 | 0.6213 | 0.8078 |
| C â€” Nonâ€‘Linear Regression Imputation | 0.7550 | 0.6056 | 0.6230 | 0.8085 |
| D â€” Listwise Deletion | 0.7784 | 0.6081 | **0.6268** | **0.8139** |

### Discussion
- **Listwise Deletion vs Imputation:** D shows slightly higher scores by **dropping incomplete rows**, which reduces noise but **shrinks** the dataset and risks **bias** if missingness is not MCAR. Imputation (Aâ€“C) **preserves data**, keeping models more representative and robust.
- **Linear vs Nonâ€‘Linear Imputation:** C â‰ˆ B â‰ˆ A. This suggests `AGE` has either **weak** or roughly **linear** relationships w.r.t. other features/label. Nonâ€‘linear benefits are minimal here.
- **Recommendation:** Prefer **Regression Imputation** over Deletion. Given similar performance and better interpretability, choose **Linear Regression Imputation** as the practical default for this dataset.

> **Oneâ€‘line takeaway:** *Keep the data, impute a single column with a simple, explainable model under MAR â†’ stable metrics and better generalization.*


---

## ğŸ—‚ï¸ Artifacts
- `DA5401_A6_template.ipynb` â€” readyâ€‘toâ€‘run notebook with all tasks.
- Generated datasets (optional):
  - `dataset_A_median.csv`
  - `dataset_B_linear_onecol.csv`
  - `dataset_C_nonlinear_onecol.csv`

---

## âœï¸ Notes & Assumptions
- MAR fraction used â‰ˆ **7%** per selected column.
- Target column is never altered.
- B & C **only** impute the **chosen column**; others stay asâ€‘is (per clarification).
- Trainâ€‘time imputer/scaler are applied **uniformly** to ensure a fair comparison.

---

**Name:** Adarsh Mahaveer Tare  
**Roll Number:** MM22B016 
